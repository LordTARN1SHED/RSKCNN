{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46bb9098-8178-42ce-9fd0-bc506bc183d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 19:27:03.621396: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "101afdea-93c2-4f28-9c7a-42ba2a94734d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 50000 files belonging to 100 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 19:27:06.473703: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-17 19:27:07.045112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 78924 MB memory:  -> device: 0, name: NVIDIA A800 80GB PCIe, pci bus id: 0000:56:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 files belonging to 100 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'resized_train_cifar100',\n",
    "    label_mode='int',  # 根据您的标签类型调整，可能需要额外处理标签文件\n",
    "    batch_size=128,\n",
    "    image_size=(224, 224))\n",
    "\n",
    "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    'resized_test_cifar100',\n",
    "    label_mode='int',\n",
    "        batch_size=128,\n",
    "    image_size=(224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e163127-ee3e-429f-8d07-6ea4cd0d297a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseConv2D(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, p, **kwargs):\n",
    "        super(SparseConv2D, self).__init__(**kwargs)\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.p = p\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(self.kernel_size, self.kernel_size, input_shape[-1], self.filters),\n",
    "                                      initializer='glorot_uniform',\n",
    "                                      trainable=True)\n",
    "        self.bias = self.add_weight(name='bias',\n",
    "                                    shape=(self.filters,),\n",
    "                                    initializer='zeros',\n",
    "                                    trainable=True)\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            mask = tf.random.uniform(shape=(self.filters,), minval=0, maxval=1)\n",
    "            mask = tf.cast(mask < self.p, dtype=tf.float32)\n",
    "            mask = tf.reshape(mask, [1, 1, 1, self.filters])\n",
    "        else:\n",
    "            mask = tf.ones([1, 1, 1, self.filters]) * self.p\n",
    "    \n",
    "        sparse_kernel = self.kernel * mask\n",
    "        conv = tf.nn.conv2d(inputs, sparse_kernel, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        return tf.nn.bias_add(conv, self.bias)\n",
    "\n",
    "    def update_p(self, new_p):\n",
    "        self.p = new_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c7948e8-ad3a-4d14-b4a9-0d12c2c1e814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 54, 54, 96)        34944     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 54, 54, 96)       384       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 26, 26, 96)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 26, 26, 256)       614656    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 26, 26, 256)      1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 384)       885120    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 12, 12, 384)       1327488   \n",
      "                                                                 \n",
      " sparse_conv2d (SparseConv2D  (None, 12, 12, 256)      884992    \n",
      " )                                                               \n",
      "                                                                 \n",
      " activation (Activation)     (None, 12, 12, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 5, 5, 256)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6400)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              26218496  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               409700    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,158,116\n",
      "Trainable params: 47,157,412\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# 使用较小的学习率和 Adam 优化器\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "def alexnet_model(input_shape=(224, 224, 3), num_classes=100):\n",
    "    model = Sequential([\n",
    "        Conv2D(96, kernel_size=11, strides=4, padding='valid', activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=3, strides=2),\n",
    "        Conv2D(256, kernel_size=5, padding='same', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=3, strides=2),\n",
    "        Conv2D(384, kernel_size=3, padding='same', activation='relu'),\n",
    "        Conv2D(384, kernel_size=3, padding='same', activation='relu'),\n",
    "        SparseConv2D(filters=256, kernel_size=3, p=1, name='sparse_conv2d'),\n",
    "        layers.Activation('relu'),\n",
    "        MaxPooling2D(pool_size=3, strides=2),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# 创建模型\n",
    "model = alexnet_model()\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 模型摘要\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48683d03-138f-4125-8f0e-2d74f892dad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpdatePSparsity(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, model, sparsity_schedule):\n",
    "        super(UpdatePSparsity, self).__init__()\n",
    "        self.model = model\n",
    "        self.sparsity_schedule = sparsity_schedule\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for layer_name, new_p in self.sparsity_schedule.items():\n",
    "            layer = self.model.get_layer(name=layer_name)\n",
    "            if epoch < len(new_p):\n",
    "                p_value = new_p[epoch]\n",
    "            else:\n",
    "                p_value = new_p[-1]  # Use the last value for epochs beyond the predefined ones\n",
    "            layer.update_p(p_value)\n",
    "            #print(f\"\\nEpoch {epoch + 1}: Updated layer {layer_name} sparsity p to {p_value}\")\n",
    "\n",
    "sparsity_schedule = {\n",
    "    'sparse_conv2d': [1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0434b5ba-7d1a-4f5e-b45e-e3b2ac4130f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-17 19:27:09.953280: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8101\n",
      "2024-07-17 19:27:11.216181: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391/391 [==============================] - 18s 38ms/step - loss: 3.9025 - accuracy: 0.1091 - val_loss: 3.6413 - val_accuracy: 0.1497\n",
      "Epoch 2/40\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 3.0982 - accuracy: 0.2397 - val_loss: 3.3722 - val_accuracy: 0.2162\n",
      "Epoch 3/40\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 2.6109 - accuracy: 0.3374 - val_loss: 2.8253 - val_accuracy: 0.3093\n",
      "Epoch 4/40\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 2.2166 - accuracy: 0.4195 - val_loss: 2.2718 - val_accuracy: 0.4152\n",
      "Epoch 5/40\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.8834 - accuracy: 0.4942 - val_loss: 2.1681 - val_accuracy: 0.4462\n",
      "Epoch 6/40\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.5653 - accuracy: 0.5662 - val_loss: 2.0068 - val_accuracy: 0.4813\n",
      "Epoch 7/40\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 1.2706 - accuracy: 0.6407 - val_loss: 1.9758 - val_accuracy: 0.5057\n",
      "Epoch 8/40\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.9919 - accuracy: 0.7096 - val_loss: 2.1366 - val_accuracy: 0.4889\n",
      "Epoch 9/40\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.7521 - accuracy: 0.7744 - val_loss: 2.0798 - val_accuracy: 0.5143\n",
      "Epoch 10/40\n",
      "391/391 [==============================] - 14s 36ms/step - loss: 0.5513 - accuracy: 0.8297 - val_loss: 2.1940 - val_accuracy: 0.5201\n",
      "Epoch 11/40\n",
      "163/391 [===========>..................] - ETA: 6s - loss: 0.4491 - accuracy: 0.8616"
     ]
    }
   ],
   "source": [
    "# 训练模型，并在验证集上验证\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=40,\n",
    "    validation_data=test_dataset,\n",
    "    callbacks=[UpdatePSparsity(model, sparsity_schedule)]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
